{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu6MUQmRUR_i"
   },
   "source": [
    "# `auto_LiRPA` Quick start tutorial http://PaperCode.cc/AutoLiRPA-Demo\n",
    "\n",
    "auto_LiRPA is a library for automatically deriving and computing bounds with linear relaxation based perturbation analysis (LiRPA) (e.g. CROWN and DeepPoly) for neural networks. LiRPA algorithms can provide guaranteed upper and lower bounds for a neural network function with perturbed inputs. These bounds are represented as linear functions with respect to the variable under perturbation. LiRPA has become an important tool in robustness verification and certified adversarial defense, and can become an useful tool for many other tasks as well.\n",
    "\n",
    "Our algorithm generalizes existing LiRPA algorithms for feed-forward neural networks to a graph algorithm on general computational graphs. We can compute LiRPA bounds on a computational graph defined by PyTorch, without any manual derivation. Our implementation is also automatically differentiable, allowing optimizing network parameters to shape the bounds into certain specifications (e.g., certified defense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjYhHLlxHI3K"
   },
   "source": [
    "## Installation & Imports\n",
    "We first install the auto_LiRPA library using pip. PyTorch 1.11+ is required. This notebook has been tested when Colab uses PyTorch 1.12.1 by default. If the default PyTorch version changes in the future, you may install PyTorch 1.12.1 by uncommenting the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./envAutoLirpa/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: sympy in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: filelock in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.2.0 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./envAutoLirpa/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./envAutoLirpa/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./envAutoLirpa/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./envAutoLirpa/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n-f-iqIZJ0Ph"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGFQGxgwK9zw"
   },
   "source": [
    "Imports for using auto_LiRPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YOFxF3WcKyfA"
   },
   "outputs": [],
   "source": [
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "from auto_LiRPA.perturbations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg9bR06eHf4Y"
   },
   "source": [
    "## Define the Computation (Neural Network)\n",
    "To begin with, we define a **18-layer ResNet** using Pytorch. The network is defined as a standard nn.module object in Pytorch, and consists of **convolutional**, **pooling** and **batch normalization** layers. We will use our auto_LiRPA library to compute bounds for this network automatically, without manual derivations of the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VtKhZTQwVPpx"
   },
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, in_planes=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, in_planes * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, in_planes * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, in_planes * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(in_planes * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(in_planes=2):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], in_planes=in_planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zPT4REmOzqL"
   },
   "source": [
    "Now we create the model, and load some pretrained parameters for demonstration. Note that this pretrained model was naturally trained so only verifiable under small perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3IrpkFtjOwg",
    "outputId": "3f7977f0-5831-470f-80ff-416453943643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-26 21:55:48--  http://download.huan-zhang.com/models/auto_lirpa/resnet18_natural.pth\n",
      "Resolving download.huan-zhang.com (download.huan-zhang.com)... 172.67.171.242, 104.21.96.11, 2606:4700:3030::6815:600b, ...\n",
      "Connecting to download.huan-zhang.com (download.huan-zhang.com)|172.67.171.242|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 208801 (204K)\n",
      "Saving to: ‘resnet18_demo.pth’\n",
      "\n",
      "resnet18_demo.pth   100%[===================>] 203.91K  46.7KB/s    in 4.4s    \n",
      "\n",
      "2024-07-26 21:55:58 (46.7 KB/s) - ‘resnet18_demo.pth’ saved [208801/208801]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(2, 4, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "# Download the model\n",
    "!wget -O resnet18_demo.pth http://download.huan-zhang.com/models/auto_lirpa/resnet18_natural.pth\n",
    "# Load pretrained weights. This pretrained model is for illustration only; it\n",
    "# does not represent state-of-the-art classification performance.\n",
    "checkpoint = torch.load(\"resnet18_demo.pth\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "#model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ1XVLnQPDCE"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "We simply use the standard CIFAR-10 dataset. We load a random image from the dataset for demonstrating the usage of our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.0 in ./envAutoLirpa/lib/python3.10/site-packages (1.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "9dac403f3e554a42a6fb0bb51bee14a3",
      "5937deb54d5a4f1f8fb79fe5e10fb568",
      "fc3d8961ec8b4239b8980cb5d88e668c",
      "4450903428af4eb0bb93974e4d1865d5",
      "58aa40b69735417b966d6871bcbc9501",
      "1c9cc5c7baad435382b6598a7163a4a0",
      "0a9c8c4554324f5395dcf23f0812c5fa",
      "194c631155a3450ebd0c89a6bbd300d9",
      "c367a0b042ca45aba74ce3079f024ba3",
      "42dca681d8c54ff9981988e0267f5055",
      "082edb7a0a46413dbdba9429f54e72b4"
     ]
    },
    "id": "XVWoWCj0nVjR",
    "outputId": "3d2ad5b2-ed3b-4ec9-d581-1fe88b74a81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Ground-truth label: 2\n",
      "Model prediction: tensor([[-2.1683, -6.2336,  5.0832, -2.8249, -3.9203, -2.3359, -2.0199, -3.7470,\n",
      "         -7.4981, -5.9163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.CIFAR10(\n",
    "    \"./data\", train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])]))\n",
    "# Choose one image from the dataset.\n",
    "idx = 123\n",
    "image = test_data[idx][0].view(1,3,32,32)\n",
    "label = data = test_data[idx][1]\n",
    "print('Ground-truth label:', label)\n",
    "print('Model prediction:', model(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXmWvpTHFN-0"
   },
   "source": [
    "## Use `auto_LiRPA` to obtain provable lower and outer bounds under perturbation\n",
    "\n",
    "There are three essential steps to use `auto_LiRPA`:\n",
    "\n",
    "1.   Wrap a predefined computation in a `nn.Module` object with `auto_LiRPA.BoundedModule`;\n",
    "2.   Define perturbation as a `BoundedTensor` (or `BoundedParameter` if you are perturbing model weights);\n",
    "3.   Use the `compute_bounds()` method to obtain lower and upper bounds of the computational graph defined in `nn.Module`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J0Uck9k6f6Yz"
   },
   "outputs": [],
   "source": [
    "# Step 1: wrap model with BoundedModule. The \"conv_mode\" option enables efficient CNN bounds on GPUs.\n",
    "bounded_model = BoundedModule(model, torch.zeros_like(image), bound_opts={\"conv_mode\": \"patches\"})\n",
    "bounded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWDY87o-gLi-",
    "outputId": "d27fab79-ca51-4450-d9e4-96507c0dab6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: tensor([[-2.1683, -6.2336,  5.0832, -2.8249, -3.9203, -2.3359, -2.0199, -3.7470,\n",
      "         -7.4981, -5.9163]])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: define perturbation. Here we use a Linf perturbation on input image.\n",
    "eps = 0.003\n",
    "norm = np.inf\n",
    "ptb = PerturbationLpNorm(norm = norm, eps = eps)\n",
    "# Input tensor is wrapped in a BoundedTensor object.\n",
    "bounded_image = BoundedTensor(image, ptb)\n",
    "# We can use BoundedTensor to get model prediction as usual. Regular forward/backward propagation is unaffected.\n",
    "print('Model prediction:', bounded_model(bounded_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7lVE11xuXVy"
   },
   "source": [
    "As you can see above, the `BoundedModule` object wrapped by `auto_LiRPA` can be used the same way as a regular Pytorch model, with a `BoundedTensor` as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VY2CZiC7g5oX",
    "outputId": "b1c85e5c-62af-4c6e-d537-461aa7427862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding method: backward (CROWN, DeepPoly)\n",
      "f_0(x_0):   -5.638 <= f_0(x_0+delta) <=    0.521\n",
      "f_1(x_0):  -10.532 <= f_1(x_0+delta) <=   -2.419\n",
      "f_2(x_0):    1.883 <= f_2(x_0+delta) <=    7.537\n",
      "f_3(x_0):   -5.327 <= f_3(x_0+delta) <=   -0.827\n",
      "f_4(x_0):   -7.217 <= f_4(x_0+delta) <=   -1.037\n",
      "f_5(x_0):   -5.238 <= f_5(x_0+delta) <=   -0.151\n",
      "f_6(x_0):   -5.686 <= f_6(x_0+delta) <=    0.118\n",
      "f_7(x_0):   -7.934 <= f_7(x_0+delta) <=   -0.303\n",
      "f_8(x_0):  -12.044 <= f_8(x_0+delta) <=   -3.793\n",
      "f_9(x_0):   -9.329 <= f_9(x_0+delta) <=   -3.074\n"
     ]
    }
   ],
   "source": [
    "# Step 3: compute bounds using the compute_bounds() method.\n",
    "print('Bounding method: backward (CROWN, DeepPoly)')\n",
    "with torch.no_grad():  # If gradients of the bounds are not needed, we can use no_grad to save memory.\n",
    "  lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='CROWN')\n",
    "\n",
    "# Auxillary function to print bounds.\n",
    "def print_bounds(lb, ub):\n",
    "    lb = lb.detach().cpu().numpy()\n",
    "    ub = ub.detach().cpu().numpy()\n",
    "    for j in range(10):\n",
    "        print(\"f_{j}(x_0): {l:8.3f} <= f_{j}(x_0+delta) <= {u:8.3f}\".format(\n",
    "            j=j, l=lb[0][j], u=ub[0][j], r=ub[0][j] - lb[0][j]))\n",
    "\n",
    "print_bounds(lb, ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG3TVOjzuplp"
   },
   "source": [
    "The backward mode perturbation analysis (an extension of [CROWN](https://https://arxiv.org/pdf/1811.00866.pdf)) provides relatively tight bounds. In this example above, the ground-truth label is 2. You can see that the model logit output for label 2 is bounded between 1.883 and 7.537, and we can guarantee that its the top-1 label under perturbation.\n",
    "\n",
    "Next, we will compute the bounds using interval bound propagation (IBP), a previous approach that can also operate on general computational graphs. However, it produces much looser and vacuous bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukIFpofCitDI",
    "outputId": "36f73373-6f82-465f-a462-0046969e7716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding method: IBP\n",
      "f_0(x_0): -23917160.000 <= f_0(x_0+delta) <= 14821592.000\n",
      "f_1(x_0): -25477750.000 <= f_1(x_0+delta) <= 16557561.000\n",
      "f_2(x_0): -18018632.000 <= f_2(x_0+delta) <= 13646840.000\n",
      "f_3(x_0): -17182968.000 <= f_3(x_0+delta) <= 9431996.000\n",
      "f_4(x_0): -22261398.000 <= f_4(x_0+delta) <= 12147502.000\n",
      "f_5(x_0): -21668396.000 <= f_5(x_0+delta) <= 12951022.000\n",
      "f_6(x_0): -24474534.000 <= f_6(x_0+delta) <= 11607182.000\n",
      "f_7(x_0): -28624076.000 <= f_7(x_0+delta) <= 17298000.000\n",
      "f_8(x_0): -29272044.000 <= f_8(x_0+delta) <= 17333460.000\n",
      "f_9(x_0): -24436316.000 <= f_9(x_0+delta) <= 12459556.000\n"
     ]
    }
   ],
   "source": [
    "# Our library also supports the interval bound propagation (IBP) based bounds,\n",
    "# but it produces much looser bounds.\n",
    "print('Bounding method: IBP')\n",
    "with torch.no_grad():\n",
    "  lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='IBP')\n",
    "\n",
    "print_bounds(lb, ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8204Z-uEtiL"
   },
   "source": [
    "## Differentiability of our bounds\n",
    "\n",
    "The bounds obtained by our `compute_bounds()` method are themselves differentiable w.r.t. input image or model parameters. We can obtain the gradients easily just as we usually do in Pytorch.  The gradients can be used for certified defense training. See our [training examples](https://github.com/KaidiXu/auto_LiRPA#basic-certified-training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEcnAkUxjMHC",
    "outputId": "09df018c-6cf1-4da5-a28f-61c6da827725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad norm: tensor(393.9586)\n"
     ]
    }
   ],
   "source": [
    "# Set model to training mode to obtain gradients.\n",
    "bounded_model.train()\n",
    "bounded_model.zero_grad()\n",
    "lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='CROWN')\n",
    "# Create a dummy scalar function for demonstrating the differentiability.\n",
    "loss = lb.sum()\n",
    "loss.backward()\n",
    "# This is the gradients of the loss w.r.t. first convolutional layer's weights:\n",
    "print('grad norm:', list(model.modules())[1].weight.grad.norm(2))\n",
    "bounded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oSAAEAkUv9a"
   },
   "source": [
    "## More examples\n",
    "\n",
    "We provide many examples of `auto_LiRPA` in our repository. You can find more details of these examples [here](https://github.com/KaidiXu/auto_LiRPA#more-working-examples). Notably, we provided the following examples for `auto_LiRPA`:\n",
    "\n",
    "1. Certified defense on CIFAR-10, **TinyImageNet** and **ImageNet** (64*64) using large scale computer vision models such as DenseNet, ResNeXt and WideResNet.\n",
    "2. Examples on using **loss fusion**, an efficient technique that scales linear relaxation based certified defense to large datasets, making certified defense training up to 1000 times faster compared to the previous approach.\n",
    "3. Examples on training verifiably robust **LSTM** and **Transformer** models on natural language processing (**NLP**) tasks.\n",
    "4. Examples on bounding network output given **model weight perturbations**. Existing frameworks can only handle perturbations on model inputs, not on model parameters (weights). This allows us to perform robustness verification or certified adversarial defense against weight perturbations. We can also train the bounds on model weights to obtain models with flat optimization landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "082edb7a0a46413dbdba9429f54e72b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a9c8c4554324f5395dcf23f0812c5fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "194c631155a3450ebd0c89a6bbd300d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c9cc5c7baad435382b6598a7163a4a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42dca681d8c54ff9981988e0267f5055": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4450903428af4eb0bb93974e4d1865d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42dca681d8c54ff9981988e0267f5055",
      "placeholder": "​",
      "style": "IPY_MODEL_082edb7a0a46413dbdba9429f54e72b4",
      "value": " 170498071/170498071 [00:13&lt;00:00, 13903721.47it/s]"
     }
    },
    "58aa40b69735417b966d6871bcbc9501": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5937deb54d5a4f1f8fb79fe5e10fb568": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c9cc5c7baad435382b6598a7163a4a0",
      "placeholder": "​",
      "style": "IPY_MODEL_0a9c8c4554324f5395dcf23f0812c5fa",
      "value": "100%"
     }
    },
    "9dac403f3e554a42a6fb0bb51bee14a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5937deb54d5a4f1f8fb79fe5e10fb568",
       "IPY_MODEL_fc3d8961ec8b4239b8980cb5d88e668c",
       "IPY_MODEL_4450903428af4eb0bb93974e4d1865d5"
      ],
      "layout": "IPY_MODEL_58aa40b69735417b966d6871bcbc9501"
     }
    },
    "c367a0b042ca45aba74ce3079f024ba3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc3d8961ec8b4239b8980cb5d88e668c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_194c631155a3450ebd0c89a6bbd300d9",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c367a0b042ca45aba74ce3079f024ba3",
      "value": 170498071
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
